{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0f74728be6e3c41931a38192cf5049ff",
     "grade": false,
     "grade_id": "cell-ad4c12a34b400e3c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "version = \"2.1.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ebee779d02f19384ce330cff4b01e0e",
     "grade": false,
     "grade_id": "cell-37e6bb32a397d91d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='toc'></a>\n",
    "# Table of Contents\n",
    "- **[Assignment Description](#Topic0)**\n",
    "- **[Topic 1 - Semi-Supervised Learning](#Topic1)**\n",
    "  - [Task 1a](#t1a)\n",
    "  - [Task 1b](#t1b)\n",
    "- **[Topic 2 - Covariate Shift](#Topic2)**\n",
    "  - [Task 2a](#t2a)\n",
    "  - [Task 2b](#t2b)\n",
    "  - [Task 2c](#t2c)\n",
    "  - [Task 2d](#t2d)\n",
    "- **[Topic 2 Helper: Scatterplot Code](#T2helper)**\n",
    "  - [Topic 2 Plot the Scatterplot](#T2plot)\n",
    "- **[Topic 3 - Imputation](#Topic3)**\n",
    "  - [Task 3a](#t3a)\n",
    "  - [Task 3b](#t3b)\n",
    "  - [Task 3c](#t3c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Either of the following is no longer\n",
    "# necessary for matplotlib in notebooks.\n",
    "# The import statement below has you covered!\n",
    "\n",
    "# %matplotlib notebook\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4772ffcd7ef2243fb0d65c0f6f9e91d0",
     "grade": false,
     "grade_id": "cell-322970f9e7c8580e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='Topic0'></a>\n",
    "# SIADS 543 Assignment 4: \n",
    "## Improving Supervised Learning with Unsupervised Methods.\n",
    "\n",
    "In this assignment, you'll explore topics from week 4: semi-supervised learning, data imputation and covariate shift, with the underlying theme being how unsupervised learning can improve supervised learning.\n",
    "\n",
    "*Please note that for autograder messages that check a list, it will typically report any problems using a list index starting at zero, i.e. the first list element is called \"element 0\".*  \n",
    "\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress all warnings only when absolutely necessary\n",
    "# Warnings are in place for a reason!\n",
    "import warnings\n",
    "\n",
    "# warnings.filterwarnings('ignore')\n",
    "# warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1c6217bdac76c63af97507acab39a948",
     "grade": false,
     "grade_id": "cell-adc369f08d1e03bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# First import some necessary libararies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "up, down = True, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "\n",
    "## Additional imports can be inlcuded here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a847ca2448dbbd248097dd18fdb2d347",
     "grade": false,
     "grade_id": "cell-9e946e896b27b328",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='Topic1'></a>\n",
    "## Topic 1 - Semi-Supervised Learning: Label Propagation (35 points total).\n",
    "\n",
    "We've provided you a dataset where some instances have labels, but most labels are missing. In this question, you'll use label propagation methods from semi-supervised learning to infer the missing labels. The application is classification: the general goal of this question is to answer: was semi-supervised learning using label propagation successful at improving the accuracy of this question's classifier?\n",
    "\n",
    "The backstory:\n",
    "\n",
    "A professor and lab assistants spend months manually classifying cancer cells as benign or malignant, and gathering a set of descriptive features based on their visual properties. Their goal is to use support vector machines to predict whether a cancer cell is benign or malignant based on its features.\n",
    "\n",
    "After painstakingly analyzing and sorting the lab samples into boxes of benign and malignant groups, the clumsy professor accidently spills a mug of artisanal, locally-sourced dark roast Arabica coffee all over the vials, erasing most of their benign/malignant labels. (Each vial and its label represent one data instance.) To add insult to injury, in trying to avoid the coffee the professor knocks the boxes to the floor and the benign/malignant groups get completely mixed together.\n",
    "\n",
    "Because of the trajectory of the coffee relative to the boxes, the professor estimates these probabilities:\n",
    "\n",
    "Probability a benign (non-malignant) cell vial (instance) had its label erased: 30%\n",
    "Probability a malignant cell vial (instance) had its label erased : 90%\n",
    "\n",
    "Their paper to the journal \"Annals of Overfitting and Data Leakage\" is due tomorrow. There are two choices they have for training their classifier and they want to maximize accuracy.\n",
    "\n",
    "`METHOD 1.` Throw away any vials whose labels were erased, and train the classifier ONLY on the much smaller but complete and accurate labeled data that remain.\n",
    "\n",
    "`METHOD 2.` Attempt to use semi-supervised learning to reconstitute the missing labels, and then train the classifier on that \"reconstituted\" dataset. An insightful colleague, an instructor for the SIADS 543 course, suggests using a label propagation algorithm in scikit-learn.\n",
    "\n",
    "Your goal for this question is to train a classifier for each of methods 1 and 2 to see how semi-supervised learning can affect classifier performance.\n",
    "\n",
    "But before you do that, run the code below to initialize the raw training and test sets. Then, instead of spilling hot coffee, you're going to simulate the label destruction by calling the provided function get_mangled_label_dataset().  \n",
    "\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61606770905f93caf7a993f43702a7a9",
     "grade": false,
     "grade_id": "cell-5f61a860d6fa6e6b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run the following startup code to load the needed (corrupted) dataset and\n",
    "# its smaller, uncorrupted counterpart.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "(X, y) = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "X_train = StandardScaler().fit_transform(X_train_raw)\n",
    "X_test = StandardScaler().fit_transform(X_test_raw)\n",
    "\n",
    "# This function takes a dataset and label set as input, and returns the following:\n",
    "#   - The X features of the subset of instances that had intact labels\n",
    "#   - The y labels of the subset of instances that had intact labels\n",
    "#   - The X features of the complete training set\n",
    "#   - The y labels of the complete training set WITH DESTROYED LABELS according to the assignment specifications.\n",
    "\n",
    "\n",
    "def get_mangled_label_dataset(X_train, y_train):\n",
    "    rng = np.random.RandomState(0)\n",
    "\n",
    "    frac_0 = 0.30  # as specified in assignment question\n",
    "    frac_1 = 0.90\n",
    "\n",
    "    X_train_0 = X_train[y_train == 0]\n",
    "    y_train_0 = y_train[y_train == 0]\n",
    "\n",
    "    X_train_1 = X_train[y_train == 1]\n",
    "    y_train_1 = y_train[y_train == 1]\n",
    "\n",
    "    X_train_complete = np.vstack((X_train_0, X_train_1))\n",
    "    y_train_complete = np.concatenate((y_train_0, y_train_1))\n",
    "\n",
    "    random_unlabeled_points_0 = rng.rand(len(y_train_0)) < frac_0\n",
    "    random_unlabeled_points_1 = rng.rand(len(y_train_1)) < frac_1\n",
    "    random_unlabeled_points = np.concatenate(\n",
    "        (random_unlabeled_points_0, random_unlabeled_points_1)\n",
    "    )\n",
    "\n",
    "    random_labeled_points_0 = np.logical_not(random_unlabeled_points_0)\n",
    "    random_labeled_points_1 = np.logical_not(random_unlabeled_points_1)\n",
    "\n",
    "    labeled_subset_X_train_0 = X_train_0[random_labeled_points_0]\n",
    "    labeled_subset_X_train_1 = X_train_1[random_labeled_points_1]\n",
    "    labeled_subset_y_train_0 = y_train_0[random_labeled_points_0]\n",
    "    labeled_subset_y_train_1 = y_train_1[random_labeled_points_1]\n",
    "\n",
    "    labeled_subset_X_train = np.vstack(\n",
    "        (labeled_subset_X_train_0, labeled_subset_X_train_1)\n",
    "    )\n",
    "    labeled_subset_y_train = np.concatenate(\n",
    "        (labeled_subset_y_train_0, labeled_subset_y_train_1)\n",
    "    )\n",
    "\n",
    "    y_train_complete_missing = np.copy(y_train_complete)\n",
    "    y_train_complete_missing[random_unlabeled_points] = -1\n",
    "\n",
    "    return (\n",
    "        labeled_subset_X_train,\n",
    "        labeled_subset_y_train,\n",
    "        X_train_complete,\n",
    "        y_train_complete_missing,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "80368d36f565114b13a037f26b2c75a9",
     "grade": false,
     "grade_id": "cell-25df8860dbac0351",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='t1a'></a>\n",
    "### Task 1a - Determine Classifier Accuracy Using Method 1 (15 points).  \n",
    "**(train only on the intact remaining labeled data).**  \n",
    "\n",
    "In preparation for this part and the next, run get_mangled_label_dataset(.) on the original training set X_train, and train a `Support Vector Classifier (SVC)` with default parameters (call this the method 1 classifier) on the resulting subset of the instances that had intact labels. Your function should return a single float: the accuracy of the method 1 classifier on the test set, i.e using X_test, y_test.  No cross-validation required: just a single test run.  \n",
    "\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3ede298d4449c2f791561d71022bd22f",
     "grade": true,
     "grade_id": "cell-3d42ec3f523c567a",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# There might be something interesting in here!\n",
    "task_id = \"1a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c48c97bb0c6a96d1cec36218d45f33df",
     "grade": false,
     "grade_id": "cell-4509fd93a08dabff",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def answer_label_propagation_a():\n",
    "    result = None\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    X_train_intact, y_train_intact, X_train_complete, y_train_complete_missing = get_mangled_label_dataset(X_train, y_train)\n",
    "    clf1 = SVC()\n",
    "    clf1.fit(X_train_intact, y_train_intact)\n",
    "\n",
    "    y_pred1 = clf1.predict(X_test)\n",
    "    acc1 = accuracy_score(y_test, y_pred1)\n",
    "    result = acc1\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell to explore your solution\n",
    "# remember to comment the function call before submitting the notebook\n",
    "\n",
    "# answer_label_propagation_a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ebc55201d37cf9451749a0428f2de97b",
     "grade": true,
     "grade_id": "cell-b2d79c086a59803c",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1a - AG tests\n",
      "Task 1a - your answer: 0.8811188811188811\n"
     ]
    }
   ],
   "source": [
    "print(f\"Task {task_id} - AG tests\")\n",
    "\n",
    "stu_ans = answer_label_propagation_a()\n",
    "print(f\"Task {task_id} - your answer: {stu_ans}\")\n",
    "\n",
    "assert isinstance(stu_ans, float), \"Task 1a: Your function should return a float.\"\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8bc8fa63ffb1ea86f2d4b285e50a8a4f",
     "grade": false,
     "grade_id": "cell-0b2793d7952be405",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='t1b'></a>\n",
    "### Task 1b - Determine Classifier Accuracy Using Method 2 (20 points).\n",
    "**(do label propagation to infer all missing labels and train on the resulting complete training set).**  \n",
    "\n",
    "Assuming you've run `get_mangled_label_dataset(.)` on the original training set X_train, now perform label propagation on the complete training set with missing labels (X_train_complete, y_train_complete_missing).  For label propagations, choose the semi-supervised method in scikit-learn that does label propagation using an affinity matrix based on the normalized graph Laplacian. With that class, **use these settings: the \"knn\" kernel with n=9 neighbors, alpha = 0.1, max_iter=30, tol=0.001**.\n",
    "\n",
    "Then, train a support vector classifier **using the SVC class** with default parameters (call this the method 2 classifier) on the resulting complete training set with inferred labels. Your function should return a single float: the accuracy of the method 2 classifier on the test set, i.e using X_test, y_test.  No cross-validation required: just a single test run.\n",
    "\n",
    "Was the label propagation effective, and save the team's journal submission?  \n",
    "\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "446325fbf623a6043f52bf57f09aa5b4",
     "grade": true,
     "grade_id": "cell-a62f850090dc8412",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Don't you wish you knew what was in here!\n",
    "task_id = \"1b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c1ac8de926787ba59a6909e86580dd0",
     "grade": false,
     "grade_id": "cell-776e288eb4e31c18",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def answer_label_propagation_b():\n",
    "    result = None\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "    from sklearn.semi_supervised import LabelSpreading\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    X_train_intact, y_train_intact, X_train_complete, y_train_complete = get_mangled_label_dataset(X_train, y_train)\n",
    "\n",
    "    label_prop_model = LabelSpreading(kernel='knn', n_neighbors=9,alpha=0.1, max_iter=30, tol=0.001)\n",
    "\n",
    "    label_prop_model.fit(X_train_complete, y_train_complete)\n",
    "\n",
    "    pred_y_train_complete = label_prop_model.predict(X_train_complete)\n",
    "\n",
    "#     pred_y_train_complete = label_prop_model.transduction_\n",
    "\n",
    "\n",
    "    clf2 = SVC()\n",
    "    clf2.fit(X_train_complete, pred_y_train_complete)\n",
    "\n",
    "    y_pred2 = clf2.predict(X_test)\n",
    "\n",
    "    acc2 = accuracy_score(y_test, y_pred2)\n",
    "    result = acc2\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell to explore your solution\n",
    "# remember to comment the function call before submitting the notebook\n",
    "\n",
    "# answer_label_propagation_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95821212c7e3d1ce62a321ca5c940734",
     "grade": true,
     "grade_id": "cell-e5a0dd64494f2fa6",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1b - AG tests\n",
      "Task 1b - your answer: 0.951048951048951\n"
     ]
    }
   ],
   "source": [
    "print(f\"Task {task_id} - AG tests\")\n",
    "\n",
    "stu_ans = answer_label_propagation_b()\n",
    "print(f\"Task {task_id} - your answer: {stu_ans}\")\n",
    "\n",
    "assert isinstance(stu_ans, float), \"Task 1b: Your function should return a float.\"\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "846919cd36e6879336bd9b01d6bfa1ee",
     "grade": false,
     "grade_id": "cell-ea7621ee18ab4382",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='Topic2'></a>\n",
    "## Topic 2 - Covariate Shift (40 points total).\n",
    "\n",
    "As discussed in this week's lecture on how unsupervised learning can help supervised learning, `covariate shift` is the name of a phenomenon in which the underlying distribution of the features for the data instances being classified using a trained model is different from the underlying distribution of the data originally used to train the classifier. This is a critical problem since the normal machine learning paradigm of estimating generalization error by minimizing test error on held-out data assumes that any future data instances to be classified are drawn from the same underlying distribution as the data used to train/test the classifier. If this assumption doesn't hold, it means your classifier may be out of date, probably giving worse predictions over time, and in need of retraining. (I use the term \"classifier\" here but the same applies if you're training/evaluating a regressor.)  Note that covariate shift is different than 'concept drift' which refers to how a *label* definition may change over time.\n",
    "\n",
    "There are many methods for detecting covariate shift. In this question we will use a simple one based on the methods you've already seen in week 1 for density estimation, and in this week's discussion (in the EM lecture) on Gaussian mixture models (GMM).\n",
    "\n",
    "Scenario: You have set up an online business selling wine. As part of a site recommender system, you build a regression model that can predict the likely user quality rating for a wine based on several features. These features summarize some of the wine's various quantitative properties (e.g. alcohol % by volume, taste rating, level of sulfites, and other chemical properties) which are provided as part of an analysis by the supplier. Your regression algorithm will try to predict the customers' perceived quality of the wine, which in turn will help you predict possible future demand and also set the wine price accordingly. Your original training data comes from the current set of wines provided by your supplier, plus labels that you gathered (in a time consuming, expensive process) from online customer ratings and in-person tasting.\n",
    "\n",
    "You won't need detailed knowledge of the features to use this data, but in case you're curious, each instance of a wine has the following real-valued features:\n",
    "\n",
    "`\"fixed acidity\"\n",
    "\"volatile acidity\"\n",
    "\"citric acid\"\n",
    "\"residual sugar\"\n",
    "\"chlorides\"\n",
    "\"free sulfur dioxide\"\n",
    "\"total sulfur dioxide\"\n",
    "\"density\"\n",
    "\"pH\"\n",
    "\"sulphates\"\n",
    "\"alcohol\"`\n",
    "\n",
    "In addition, there's a final column which is the instance label, i.e. the quality rating:\n",
    "   `\"quality\"`  (This is an ordinal quality rating from 1-10 from a human judge: higher is better quality.)\n",
    "\n",
    "Before starting, run the preamble code below to load the various dataset(s) you'll need.\n",
    "\n",
    "**NOTE: As always, use `random_state=42` for any regressor, mixture model, or other scikit-learn call that takes it as an argument.**  \n",
    "\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f4b921ca51ac9b04a057227f1243e267",
     "grade": false,
     "grade_id": "cell-6b9166a5c16e64b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load your base training set that will be used to train the starting classifier.\n",
    "df_base = pd.read_csv(\"assets/wine-base-dataset.csv\")\n",
    "\n",
    "# take all columns except the last\n",
    "X_base = df_base.iloc[:, :-1]\n",
    "\n",
    "# take just the last column (the quality score, which is the regression target)\n",
    "y_base = df_base.iloc[:, -1]\n",
    "\n",
    "X_base_normalized = StandardScaler().fit_transform(X_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6d8a97b56f375cb87db7f42cac9b9d87",
     "grade": false,
     "grade_id": "cell-b6ea65022f6292a1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='t2a'></a>\n",
    "### Task 2a - Compute the baseline regression score (10 points).\n",
    "**Your wine quality classifier using the existing base dataset.**  \n",
    "\n",
    "You decide to start by using a random forest regressor for your quality predictions. Using your existing complete X_base_normalized dataset, compute the mean five-fold cross-validation score achieved by a default random forest regressor.  Your function should return a `float` representing this mean score.  \n",
    "\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa81477d608c4c48c3976323dbff045e",
     "grade": true,
     "grade_id": "cell-426de7b5e5c3c8ea",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This might be interesting.\n",
    "task_id = \"2a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d2ff17355dcbb4dc0cf7f569ad250d37",
     "grade": false,
     "grade_id": "cell-8eea2e5dcf829c94",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def answer_covariate_shift_a():\n",
    "    result = None\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    regressor = RandomForestRegressor(random_state=42)\n",
    "    scores = cross_val_score(regressor, X_base_normalized, y_base, cv=5)\n",
    "    result = np.mean(scores)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell to explore your solution\n",
    "# remember to comment the function call before submitting the notebook\n",
    "\n",
    "# answer_covariate_shift_a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a4d6f03a4635c58754767e0402e5750",
     "grade": true,
     "grade_id": "cell-2079bdf62870ff23",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2a - AG tests\n",
      "Task 2a - your answer: 0.3081286839917401\n"
     ]
    }
   ],
   "source": [
    "print(f\"Task {task_id} - AG tests\")\n",
    "\n",
    "stu_ans = answer_covariate_shift_a()\n",
    "print(f\"Task {task_id} - your answer: {stu_ans}\")\n",
    "\n",
    "assert isinstance(stu_ans, float), \"Task 2a: Your function should return a float.\"\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "567ae79ae70879026f351bf9b9a12f99",
     "grade": false,
     "grade_id": "cell-76a2a8079c3a66c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='t2b'></a>\n",
    "### Task 2b - Fit a Gaussian Mixture Model to the base dataset (10 points).\n",
    "**Evaluate the average log probability density of the base dataset under this GMM.**  \n",
    "\n",
    "Create an instance of the sklearn.mixture class GaussianMixture with 5 components and a full covariance matrix, and fit it using the base dataset.\n",
    "\n",
    "You can refer to the week 1 notebook density estimation examples for the method you use to fit a mixture model, and to compute the log probability density of a set X of instances.\n",
    "\n",
    "Your function should return a `float` containing the mean log probability of the base dataset instances under this GMM.  \n",
    "\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de6e35ba80030820ca33ec8de08e9ffd",
     "grade": true,
     "grade_id": "cell-f303c4a1882c0afa",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This must be the good stuff!\n",
    "task_id = \"2b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f3815a7b9d5bdb6daeda69e2ec9447fd",
     "grade": false,
     "grade_id": "cell-205448801b462753",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def answer_covariate_shift_b():\n",
    "    result = None\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    gmm = GaussianMixture(n_components=5,random_state=42, covariance_type='full')\n",
    "    gmm.fit(X_base_normalized)\n",
    "    log_prob = gmm.score_samples(X_base_normalized)\n",
    "    result = np.mean(log_prob)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell to explore your solution\n",
    "# remember to comment the function call before submitting the notebook\n",
    "\n",
    "# answer_covariate_shift_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2414fafb58b2cb1fe3a3bd2a0102f6b",
     "grade": true,
     "grade_id": "cell-39765eec0d59ac88",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2b - AG tests\n",
      "Task 2b - your answer: -10.100136924918216\n"
     ]
    }
   ],
   "source": [
    "print(f\"Task {task_id} - AG tests\")\n",
    "\n",
    "stu_ans = answer_covariate_shift_b()\n",
    "print(f\"Task {task_id} - your answer: {stu_ans}\")\n",
    "\n",
    "assert isinstance(stu_ans, float), \"Task 2b: Your function should return a float.\"\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bcd1728e3b5a7cdf1602920a84fe723c",
     "grade": false,
     "grade_id": "cell-707e23fd32aec2f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='t2c'></a>\n",
    "### Task 2c - Compute the mean log probability of a new items dataset using the specified GMM (5 points).\n",
    "**The new dataset does not have labels!**  \n",
    "\n",
    "A new wine inventory list comes in from your supplier in the file `wine-newitems-dataset.csv`.  These are the wines the supplier expects to be able to deliver in the next month and they come with the usual features.\n",
    "\n",
    "You decide to check for covariate shift by seeing how likely, on average, these new items are in the GMM trained with the (old) base dataset. If the new items are really unlikely, it may be time to consider making the effort to get labels for some or all of these new items in order to create an updated training set. (Okay - if you look closely the newitems dataset already has a final quality label column you will use for Part 2d, but for the purposes of this question, you can ignore that column.)\n",
    "\n",
    "Your function should return a float with the mean log probability of the new item samples under the base GMM you fit in Part 2b (trained with the base dataset).\n",
    "\n",
    "Compare this to your answer from Part 2b, the mean log probability of the (old) base samples under the same GMM. \n",
    "What is the difference? (Note that log probabilities will be negative numbers, since probabilities are between 0 and 1.)\n",
    "\n",
    "Run the code below to load and normalized a dataset of new items (that are unlabeled).  \n",
    "\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5946b9826f4745e37c91ce9019f5e89a",
     "grade": false,
     "grade_id": "cell-9faf0f2bf817af8c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Use this code to load the new items dataset\n",
    "\n",
    "df_newitems = pd.read_csv(\"assets/wine-newitems-dataset.csv\")\n",
    "\n",
    "# labels only\n",
    "y_newitems = df_newitems.iloc[:, -1]\n",
    "\n",
    "# drop the label from df\n",
    "X_newitems = df_newitems.iloc[:, :-1]\n",
    "\n",
    "# scale X_newitems\n",
    "X_newitems_normalized = StandardScaler().fit_transform(X_newitems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f634601c41c788c6de08e32181e1720",
     "grade": true,
     "grade_id": "cell-96d46b0577a728c6",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# What do we have here?\n",
    "task_id = \"2c\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3b18c305d847dbe0eccc2808dbfc791b",
     "grade": false,
     "grade_id": "cell-dcf0303572bde9a4",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "\n",
    "\n",
    "def answer_covariate_shift_c():\n",
    "    result = None\n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    gmm = GaussianMixture(n_components=5,random_state=42, covariance_type='full')\n",
    "    gmm.fit(X_base_normalized)\n",
    "    log_prob = gmm.score_samples(X_newitems_normalized)\n",
    "    result = np.mean(log_prob)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell to explore your solution\n",
    "# remember to comment the function call before submitting the notebook\n",
    "\n",
    "# answer_covariate_shift_c()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54495b2e92ff5c8897d9aa2630a783ab",
     "grade": true,
     "grade_id": "cell-36c9d4d11835cbb5",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2c - AG tests\n",
      "Task 2c - your answer: -17.18711776688866\n"
     ]
    }
   ],
   "source": [
    "print(f\"Task {task_id} - AG tests\")\n",
    "\n",
    "stu_ans = answer_covariate_shift_c()\n",
    "print(f\"Task {task_id} - your answer: {stu_ans}\")\n",
    "\n",
    "assert isinstance(stu_ans, float), \"Task 2c: Your function should return a float.\"\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e309a5695fba05d6211c4dde47a0bc7e",
     "grade": false,
     "grade_id": "cell-d470a57861fe3da7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='t2d'></a>\n",
    "## Task 2d - Score original items vs new items expanded datasets (15 points).\n",
    "**Retrain your original classifier by incorporating the new labeled training data.**  \n",
    "\n",
    "Based on the results of the previous analysis, you decide to retrain your random forest regressor. (You pay a survey firm to get judges who provide the quality scores.)\n",
    "\n",
    "First create a new combined dataset by appending the `newitems` labeled dataset *after* the end of the `base` labeled dataset.\n",
    "\n",
    "Now that you have the new item labels you should compute two numbers here:\n",
    "\n",
    "(i) The score you get when using X_newitems_normalized and y_newitems as the test set with a RandomForest regressor trained on the original base dataset only (no cross-validation, just one run).  This is the regression score you would have gotten on the new items if you hadn't retrained the quality prediction model.\n",
    "\n",
    "(ii) Compute the mean five-fold cross-validation score achieved by a default random forest regressor that uses your updated dataset.\n",
    "\n",
    "Your function should return a tuple with two `float` elements, containing the results of (i) and (ii), in that order.\n",
    "Comparing (i) with (ii), was it worth getting the new labeled data?  \n",
    "\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7b8eb9ee1fca2350964c2d6abc83d6a",
     "grade": true,
     "grade_id": "cell-50a086cbed01bf9f",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hmmm, what have we here?\n",
    "task_id = \"2d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "692970b8f513fb3565bd230dad9be112",
     "grade": false,
     "grade_id": "cell-a80d7fa8ad732a4c",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def answer_covariate_shift_d():\n",
    "    result = None\n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.metrics import accuracy_score,f1_score\n",
    "    regressor = RandomForestRegressor(random_state=42)\n",
    "    regressor.fit(X_base_normalized, y_base)\n",
    "#     scores = cross_val_score(regressor, X_newitems_normalized, y_newitems)\n",
    "#     result1 = np.mean(scores)\n",
    "    pred = regressor.predict(X_newitems_normalized)\n",
    "#     result1 = accuracy_score(y_newitems, pred)\n",
    "    result1 = regressor.score(X_newitems_normalized, y_newitems)\n",
    "    \n",
    "\n",
    "\n",
    "    X_final = np.vstack((X_base_normalized, X_newitems_normalized))\n",
    "    y_final = pd.concat([y_base, y_newitems])\n",
    "    regressor = RandomForestRegressor(random_state=42)\n",
    "    scores = cross_val_score(regressor, X_final, y_final, cv=5)\n",
    "    result2 = np.mean(scores)\n",
    "    result = (result1, result2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell to explore your solution\n",
    "# remember to comment the function call before submitting the notebook\n",
    "\n",
    "# answer_covariate_shift_d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9fa71ed417a97a5b3f2b45cd759c45aa",
     "grade": true,
     "grade_id": "cell-c011280a8630d5c3",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2d - AG tests\n",
      "Task 2d - your answer: (0.04764901800696186, 0.28193821414596937)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Task {task_id} - AG tests\")\n",
    "\n",
    "stu_ans = answer_covariate_shift_d()\n",
    "print(f\"Task {task_id} - your answer: {stu_ans}\")\n",
    "\n",
    "assert isinstance(stu_ans, tuple), \"Task 2d: Your function should return a tuple.\"\n",
    "assert len(stu_ans) == 2, \"Task 2d: Your tuple should have two floats.\"\n",
    "assert isinstance(stu_ans[0], float), \"Task 2d: Element 0 should be a float.\"\n",
    "assert isinstance(stu_ans[1], float), \"Task 2d: Element 1 should be a float.\"\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c9fb13b8d157205f3cc6904b9e4f3d71",
     "grade": false,
     "grade_id": "cell-1d9ca15691990b55",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='T2helper'></a>\n",
    "## Task 2 Helper: Scatterplot Code\n",
    " - You can use this plotting code to compare visually the distributions of the base and newitems datasets to get more intuition about how they might differ.  \n",
    "\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b586c17c1788b2cc0313888fda39eee0",
     "grade": false,
     "grade_id": "cell-87efb0a678ee0ef8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "def plot_labelled_scatterT(X, y, class_labels):\n",
    "    num_labels = len(class_labels)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "\n",
    "    marker_array = [\"o\", \"^\", \"*\"]\n",
    "    color_array = [\n",
    "        \"#FFFF00\",\n",
    "        \"#00AAFF\",\n",
    "        \"#000000\",\n",
    "        \"#FF00AA\",\n",
    "        \"#00FF00\",\n",
    "        \"#FFFFAA\",\n",
    "        \"#0000FF\",\n",
    "        \"#AAAA00\",\n",
    "        \"#00AAAA\",\n",
    "    ]\n",
    "    cmap_bold = ListedColormap(color_array)\n",
    "    bnorm = BoundaryNorm(np.arange(0, num_labels + 1, 1), ncolors=num_labels)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(\n",
    "        X[:, 0],\n",
    "        X[:, 1],\n",
    "        s=65,\n",
    "        c=y,\n",
    "        cmap=cmap_bold,\n",
    "        norm=bnorm,\n",
    "        alpha=0.40,\n",
    "        edgecolor=\"black\",\n",
    "        lw=1,\n",
    "    )\n",
    "\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "\n",
    "    h = []\n",
    "    for c in range(0, num_labels):\n",
    "        h.append(mpatches.Patch(color=color_array[c], label=class_labels[c]))\n",
    "    plt.legend(handles=h)\n",
    "    plt.grid(alpha=0.15)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d8f845451e162efaa8f2e5529e07d4e",
     "grade": false,
     "grade_id": "cell-e82a5dcc2ef06d27",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Question 2: You can use MDS to visualize the base and newitems distributions\n",
    "# This is one way to quickly check for the possibility of covariate shift.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "X_binary = np.vstack((X_base_normalized, X_newitems_normalized))\n",
    "y_binary = np.concatenate((np.ones(y_base.shape[0]) - 1, np.ones(y_newitems.shape[0])))\n",
    "\n",
    "# Subsample just a part of the original data for efficiency reasons with MDS\n",
    "sample = np.random.choice(X_binary.shape[0], 500, replace=False)\n",
    "\n",
    "dr = MDS(n_components=2, random_state=42)\n",
    "X_low = dr.fit_transform(X_binary[sample])\n",
    "\n",
    "\n",
    "# Uncomment this code to generate the labelled scatterT plot\n",
    "# remember to recomment before submitting\n",
    "\n",
    "# plot_labelled_scatterT(X_low, y_binary[sample], [\"1: base\", \"2: newitems\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a17287107d0f59c48087ec79f06039dd",
     "grade": false,
     "grade_id": "cell-1756b645ba92d7c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='T2plot'></a>\n",
    "### Plot the Scatterplot\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this code to generate the labelled scatterT plot\n",
    "# remember to recomment before submitting\n",
    "\n",
    "# plot_labelled_scatterT(X_low, y_binary[sample], [\"1: base\", \"2: newitems\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8901ec9c98d1138a45a979eed010d0ce",
     "grade": false,
     "grade_id": "cell-77916a8237c351a9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='Topic3'></a>\n",
    "## Topic 3 - Imputation of Missing Values (25 points total).\n",
    "\n",
    "In addition to trying to infer missing *labels* with semi-supervised learning, we can use the Imputer classes of sklearn to fill in missing *values* of the *features*. In this question, you'll use the same dataset as in Question 1, but will look more closely at the process of imputing missing values. Please refer to the week 4 reading, notebook and data imputation lecture for examples of code on how to use an Imputer class (including optionally tied to a Regression class via a pipeline object). \n",
    "\n",
    "### Reminder\n",
    "**Set `random_state=42` for any estimator or other sklearn objects that accept the parameter.**  \n",
    "\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "573c38f6d696256d3f8d75d4626157e7",
     "grade": false,
     "grade_id": "cell-4d0de3e558e546a6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this code before proceeding.\n",
    "# Please note it makes a copy of the original data and does not modify the originals.\n",
    "\n",
    "CV_SPLITS = 5\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "### Load the full dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "(X_full_raw, y_full) = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "\n",
    "### Now we create new subset datasets X_full and X_missing with just two features\n",
    "### for purposes of this question. Random values in X_missing will be marked with NaN,\n",
    "### (although for those who look closely at the code, you'll see the missingness will\n",
    "### not necessarily be completely at random, which will become important later!)\n",
    "X_full = X_full_raw[:, (14, 25)]\n",
    "\n",
    "# We create a train/test split although this is for internal statistical reasons\n",
    "# and the split will not be used in the questions themselves.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, random_state=0)\n",
    "\n",
    "mu = X_train.mean(axis=0)\n",
    "MU_WIDTH = 2\n",
    "\n",
    "n_samples = X_train.shape[0]\n",
    "X_train_missing = X_train.copy()\n",
    "u = rng.uniform(low=mu[0], high=mu[0] * MU_WIDTH, size=(n_samples,))\n",
    "X_train_missing[np.where(u < X_train[:, 0])[0], 0] = np.nan\n",
    "u = rng.uniform(low=mu[1], high=mu[1] * MU_WIDTH, size=(n_samples,))\n",
    "X_train_missing[np.where(u < X_train[:, 1])[0], 1] = np.nan\n",
    "\n",
    "n_samples = X_test.shape[0]\n",
    "mu = X_test.mean(axis=0)\n",
    "X_test_missing = X_test.copy()\n",
    "u = rng.uniform(low=mu[0], high=mu[0] * MU_WIDTH, size=(n_samples,))\n",
    "X_test_missing[np.where(u < X_test[:, 0])[0], 0] = np.nan\n",
    "u = rng.uniform(low=mu[1], high=mu[1] * MU_WIDTH, size=(n_samples,))\n",
    "X_test_missing[np.where(u < X_test[:, 1])[0], 1] = np.nan\n",
    "\n",
    "## The final \"missing data\" version of the full dataset:\n",
    "X_missing = np.vstack((X_train_missing, X_test_missing))\n",
    "y_missing = np.append(y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e55df3c50b0e451ea53d51049e7ca8b",
     "grade": false,
     "grade_id": "cell-875a02aaaaf79fa6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='t3a'></a>\n",
    "## Task 3a - Create a full data prediction baseline model (5 points).\n",
    "With the full dataset X_full, y_full, and using LogisticRegression with default parameters, write a function to compute the mean cross-validation classifier score on the full dataset. Return that result as a single float.  \n",
    "\n",
    "### Setup specifics\n",
    "- use X_full for \"the full dataset\"\n",
    "- use X_missing & y_missing for the missing values dataset: this contains random np.nan values\n",
    "- 5 folds for the cross-validation.\n",
    "\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a953adb820d19ded980e553c28ea3877",
     "grade": true,
     "grade_id": "cell-868a80ed8344e1c1",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# If you could only open this door.\n",
    "task_id = \"3a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "27917f8ec45dbabecaa2381a950b201d",
     "grade": false,
     "grade_id": "cell-a1ebb9bf2bee0194",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def answer_imputation_a():\n",
    "    result = None\n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    logreg = LogisticRegression()\n",
    "    cv_scores = cross_val_score(logreg, X_full,y_full, cv=5)\n",
    "    result = cv_scores.mean()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell to explore your solution\n",
    "# remember to comment the function call before submitting the notebook\n",
    "\n",
    "# answer_imputation_a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d782c0f8de061bac857ae4a943f5449a",
     "grade": true,
     "grade_id": "cell-312a3feb6621676c",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3a - AG tests\n",
      "Task 3a - your answer: 0.7890855457227139\n"
     ]
    }
   ],
   "source": [
    "print(f\"Task {task_id} - AG tests\")\n",
    "\n",
    "stu_ans = answer_imputation_a()\n",
    "print(f\"Task {task_id} - your answer: {stu_ans}\")\n",
    "\n",
    "assert isinstance(stu_ans, float), \"Task 3a: Your function should return a float.\"\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75739193b09f1aed2fcf773788b0626b",
     "grade": false,
     "grade_id": "cell-e42bb8429f5e9de5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='t3b'></a>\n",
    "## Task 3b - Simple Imputation, with and without adding Indicator Variables (10 points).\n",
    "\n",
    "Not only will a scikit-learn Imputer object fill in the missing values of the input dataset, but setting the Imputer option `add_indicator=True` will cause extra binary indicator features to be added to the imputed dataset: a set of binary variables, one per original feature, for remembering which features *in that row* were imputed. Why would this be useful?  It's useful because if the values are *not* missing completely at random and e.g. are related to the range of a variable -- for example, if people with higher incomes are less likely to reveal them in a survey -- then by adding these extra binary features, the classifier has at least a chance for learning something about the relationship of how the 'missingness' might relate to the target value. In other words, the very fact that a value is missing can tell the classifier at least something potentially about its range or distribution, which might be useful for learning. \n",
    "\n",
    "In case you're curious, we have designed a \"missing not completely at random\" (i.e conditionally random) dataset below based on the mean value in each column: the likelihood of a missing value increases with the variable magnitude. \n",
    "\n",
    "The purpose of this question is to see what the effect is on classifier accuracy in such a scenario from setting add_indicator = True to add those extra binary indicator features. (It's instructive to take a look at the output of the Imputer object on your own to see how the indicator variables are added in response to missing values.)\n",
    "\n",
    "Write a function that:\n",
    "\n",
    "(i)  Uses a SimpleImputer with parameters `missing_values = np.nan`, mean imputation strategy, and `add_indicator = False`, using a pipeline to connect the SimpleImputer to a LogisticRegression classifier with default parameters. With this composite pipeline estimator, compute the mean five-fold cross-validation classifier score on the X_missing dataset.\n",
    "\n",
    "(ii) Runs exactly the same steps as above, but with the SimpleImputer's parameter `add_indicator = True`.\n",
    "\n",
    "Your function should return a tuple of two floats: the results of (i) and (ii) in that order.  \n",
    "\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b6b4d174f0d1b640abba0c8947bfb2a5",
     "grade": true,
     "grade_id": "cell-743ec68957c30d77",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Just a little hint???\n",
    "task_id = \"3b\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4770337edc10370faefdc52735ad0454",
     "grade": false,
     "grade_id": "cell-b6083ec014bcb5bb",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def answer_imputation_b():\n",
    "    result = None\n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.pipeline import make_pipeline\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy='mean', add_indicator=False)\n",
    "    logreg = LogisticRegression()\n",
    "    pipeline = make_pipeline(imputer, logreg)\n",
    "    cv_scores = cross_val_score(pipeline, X_missing, y_missing, cv=5)\n",
    "    result1 = cv_scores.mean()\n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy='mean', add_indicator=True)\n",
    "    logreg = LogisticRegression()\n",
    "    pipeline = make_pipeline(imputer, logreg)\n",
    "    cv_scores = cross_val_score(pipeline, X_missing, y_missing, cv=5)\n",
    "    result2 = cv_scores.mean()\n",
    "    result = (result1, result2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell to explore your solution\n",
    "# remember to comment the function call before submitting the notebook\n",
    "\n",
    "# answer_imputation_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f96ce10688f7ffba9a89da9ca96a007b",
     "grade": true,
     "grade_id": "cell-c5dad89521575d17",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3b - AG tests\n",
      "Task 3b - your answer: (0.6765719608756404, 0.7627387051700046)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Task {task_id} - AG tests\")\n",
    "\n",
    "stu_ans = answer_imputation_b()\n",
    "print(f\"Task {task_id} - your answer: {stu_ans}\")\n",
    "\n",
    "assert isinstance(stu_ans, tuple), \"Task 3b: Your function should return a tuple.\"\n",
    "assert len(stu_ans) == 2, \"Task 3b: Your tuple should have two floats.\"\n",
    "assert isinstance(stu_ans[0], float), \"Task 3b: Element 0 should be a float.\"\n",
    "assert isinstance(stu_ans[1], float), \"Task 3b: Element 1 should be a float.\"\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3d602f3977a96679714be913d3471477",
     "grade": false,
     "grade_id": "cell-447c555f82e5a0e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='t3c'></a>\n",
    "## Task 3c - Iterative Imputation with Indicator Variables (10 points).\n",
    "\n",
    "Write a function that uses code very similar to the one you created in the previous step, but modified to use an IterativeImputer class, with the estimator property set to `DecisionTreeRegressor(random_state = 42)` so that it will use regression trees to perform the imputation.  \n",
    "\n",
    "This custom estimator property is one example of the IterativeImputer's flexibility. Set `add_indicator=True` to add the extra imputation indicator features for the LogisticRegression classifier (default settings). Compute the mean five-fold cross-validation classifier score on the X_missing dataset and return that from your function.  \n",
    "\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2dc087d154ba8e1bd75e7a8e53c89896",
     "grade": true,
     "grade_id": "cell-4867c6a7716734e0",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Nothing to see here, move along...\n",
    "task_id = \"3c\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "613ed6f35d4ef2983e74bf6922aaab0f",
     "grade": false,
     "grade_id": "cell-3a06c869fbaeb3d8",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def answer_imputation_c():\n",
    "    result = None\n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "\n",
    "    from sklearn.experimental import enable_iterative_imputer\n",
    "    from sklearn.impute import IterativeImputer\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.pipeline import make_pipeline\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    imputer = IterativeImputer(estimator=DecisionTreeRegressor(random_state = 42), add_indicator=True)\n",
    "    logistic = LogisticRegression()\n",
    "    # scaler = StandardScaler()\n",
    "    # pipe = make_pipeline(imputer, scaler, logistic)\n",
    "    pipe = make_pipeline(imputer,logistic)\n",
    "    cv_scores = cross_val_score(pipe, X_missing, y_missing, cv=5)\n",
    "    result = cv_scores.mean()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.experimental import enable_iterative_imputer\n",
    "# from sklearn.impute import IterativeImputer\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# imputer = IterativeImputer(estimator=DecisionTreeRegressor(random_state = 42), add_indicator=True)\n",
    "# logistic = LogisticRegression()\n",
    "# # scaler = StandardScaler()\n",
    "# # pipe = make_pipeline(imputer, scaler, logistic)\n",
    "# pipe = make_pipeline(imputer,logistic)\n",
    "# cv_scores = cross_val_score(pipe, X_missing, y_missing, cv=5)\n",
    "# result = cv_scores.mean()\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell to explore your solution\n",
    "# remember to comment the function call before submitting the notebook\n",
    "\n",
    "# answer_imputation_c()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "888acf3c89beb261d2bf25aed29a25b0",
     "grade": true,
     "grade_id": "cell-2f75adf88cde04cd",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3c - AG tests\n",
      "Task 3c - your answer: 0.7609843192050924\n"
     ]
    }
   ],
   "source": [
    "print(f\"Task {task_id} - AG tests\")\n",
    "\n",
    "stu_ans = answer_imputation_c()\n",
    "print(f\"Task {task_id} - your answer: {stu_ans}\")\n",
    "\n",
    "assert isinstance(stu_ans, float), \"Task 3c: Your function should return a float.\"\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4f6857a127f0cf7606bdae64d99c1cad",
     "grade": false,
     "grade_id": "cell-c916271f5c220cf2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a href='#toc'>TOC</a>"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "mads_unsupervised_learning_v1_assignment4"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
